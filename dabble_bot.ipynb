{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abaft',\n",
       " 'abalone',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandonment']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('C:\\\\Users\\\\benak\\\\Documents\\\\More Documents\\\\words.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  (n) the vertical force exerted by a mass as a result of gravity\n",
      "2:  (n) sports equipment used in calisthenic exercises and weightlifting; it is not attached to anything and is raised and lowered by use of the hands and arms\n",
      "3:  (n) the relative importance granted to something\n",
      "4:  (n) an artifact that is heavy\n",
      "5:  (n) an oppressive feeling of heavy force\n",
      "6:  (n) a system of units used to express the weight of something\n",
      "7:  (n) a unit used to measure weight\n",
      "8:  (n) (statistics) a coefficient assigned to elements of a frequency distribution in order to represent their relative importance\n",
      "9:  (v) weight down with a load\n",
      "10:  (v) present with a bias\n"
     ]
    }
   ],
   "source": [
    "# sample definitions for the word: weight\n",
    "syns = wn.synsets('weight')\n",
    "num = 0\n",
    "for s in syns:\n",
    "    num += 1\n",
    "    print(f'{num}: ', f'({s.pos()})', s.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Words Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n",
      "4665\n"
     ]
    }
   ],
   "source": [
    "wsample = []\n",
    "undefined = []\n",
    "ix = torch.randperm(len(words))[:5000]\n",
    "for ix in ix: wsample.append(words[ix])\n",
    "for w in wsample:\n",
    "    if len(wn.synsets(w)) < 1: \n",
    "        undefined.append(w)      # create set of words without a wordnet definition \n",
    "        wsample.remove(w)        # remove undefined words from the sample\n",
    "        \n",
    "print(len(undefined))\n",
    "print(len(wsample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions for all words in wsample\n",
    "definitions = [s.definition() for w in wsample for s in wn.synsets(w)]\n",
    "\n",
    "# remove punctuations from definitions and append ' . ' sentence-end token\n",
    "trim_definitions = [''.join(d).translate(str.maketrans('', '', string.punctuation)) + ' . ' for d in definitions]\n",
    "trim_defstring = ''.join(trim_definitions)    # join punctuation-free definitions into a string\n",
    "\n",
    "# create vocab list of all individual words that appear in the sample set of definitions\n",
    "vocab = sorted(list((dict.fromkeys(trim_defstring.split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions in sample: \n",
      "  13833\n",
      "Distinct words in sampled definitions: \n",
      "  126275\n",
      "Unique words in sampled definitions: \n",
      "  Vocab set: 12960\n"
     ]
    }
   ],
   "source": [
    "print('Definitions in sample: \\n ', len(definitions))\n",
    "print('Distinct words in sampled definitions: \\n ', len(trim_defstring.split()))\n",
    "print('Unique words in sampled definitions: \\n  Vocab set:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of rare words in the sample vocab set (words appearing only once in the sample of definitions)\n",
    "counts = []\n",
    "for word in vocab:\n",
    "    counts += [trim_defstring.count(word)]\n",
    "\n",
    "idk = []\n",
    "for i in range(len(counts)):\n",
    "    if counts[i] < 2: idk.append(i)\n",
    "rare_words = [vocab[ix] for ix in idk]\n",
    "len(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab set excl rare words: 8034\n"
     ]
    }
   ],
   "source": [
    "# remove all rare words from the vocab list\n",
    "trim_vocab = vocab\n",
    "for w in rare_words:\n",
    "    trim_vocab.remove(w)\n",
    "print('Vocab set excl rare words:', len(trim_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a stone coffin usually bearing sculpture or inscriptions . ',\n",
       " 'the syllable naming the first tonic note of any major scale in solmization . ',\n",
       " 'a crafty and involved plot to achieve your usually sinister ends . ',\n",
       " 'a clandestine love affair . ',\n",
       " 'cause to be interested or curious . ',\n",
       " 'form intrigues for in an underhand manner . ',\n",
       " 'press tightly together or cram . ',\n",
       " 'filled with great numbers crowded together . ',\n",
       " 'show to be right by providing justification or proof . ',\n",
       " 'maintain uphold or defend . ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rare words from the sample definitions\n",
    "trimmer_defs = trim_defstring.split()\n",
    "for w in rare_words:\n",
    "    trimmer_defs.remove(w)\n",
    "trimmer_defs = ' '.join(trimmer_defs).split(' . ')\n",
    "trimmer_defs = [d + ' . ' for d in trimmer_defs]\n",
    "trimmer_defs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i,s in enumerate(trim_vocab)}    # word-to-integer mapping dictionary\n",
    "# stoi['.'] = 0                                    # adding period\n",
    "itos = {i:s for s,i in stoi.items()}               # integer-to-word mapping dictionary\n",
    "\n",
    "enc = lambda s: [stoi[c] for c in s]           # encoder\n",
    "dec = lambda l: ''.join([itos[i] for i in l])  # decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8                     # context length: the number of words used to predict the next word\n",
    "\n",
    "# function for initializing the dataset, will make subsetting easier\n",
    "def build_dataset(dat):\n",
    "    X, Y = [], []\n",
    "    for definition in dat:\n",
    "        context = [0] * block_size     # zeros-list of length block_size\n",
    "        for word in definition.split():\n",
    "            ix = stoi[word]              # retrieve word index integer from stoi dict\n",
    "            X.append(context)            # lengthen X by the context list\n",
    "            Y.append(ix)                 # append word index to Y\n",
    "            context = context[1:] + [ix]      # shift context window to include ix (dropping former context[0] entry)\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99024, 8]) torch.Size([99024])\n",
      "torch.Size([12454, 8]) torch.Size([12454])\n",
      "torch.Size([12328, 8]) torch.Size([12328])\n"
     ]
    }
   ],
   "source": [
    "# randomly generate word subsets\n",
    "\n",
    "random.seed(42)                 # set seed for consistency\n",
    "random.shuffle(trimmer_defs)           # randomize dataset arrangement\n",
    "n1 = int(0.8*len(trimmer_defs))\n",
    "n2 = int(0.9*len(trimmer_defs))\n",
    "\n",
    "Xtr, Ytr = build_dataset(trimmer_defs[:n1])       # training set inputs and labels         (80%)\n",
    "Xdev, Ydev = build_dataset(trimmer_defs[n1:n2])   # validation (dev) set inputs and labels (10%)\n",
    "Xte, Yte = build_dataset(trimmer_defs[n2:])       # test set inputs and labels             (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlattenConsecutive Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cribbed from lecture bc the nn.Flatten function doesn't work the way that I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive:\n",
    "  \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization with WaveNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilated causal convolution layers.....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8, 20])\n",
      "torch.Size([32, 160])\n",
      "torch.Size([100, 160])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0082, -0.0670,  0.0457,  ...,  0.0166, -0.0564,  0.0098],\n",
      "        [ 0.0578, -0.0471, -0.0754,  ..., -0.0726, -0.0772,  0.0773],\n",
      "        [-0.0609, -0.0240,  0.0330,  ...,  0.0148,  0.0336, -0.0458],\n",
      "        ...,\n",
      "        [ 0.0426,  0.0729, -0.0608,  ...,  0.0285, -0.0675,  0.0168],\n",
      "        [-0.0065,  0.0755,  0.0077,  ...,  0.0140,  0.0696, -0.0529],\n",
      "        [-0.0145,  0.0482,  0.0040,  ...,  0.0210, -0.0744, -0.0779]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (32, ))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]   \n",
    "print(Xb.shape)\n",
    "x = nn.Embedding(vocab_size, n_emb)(Xb)\n",
    "print(x.shape)\n",
    "xf = nn.Flatten(1, 2)(x)\n",
    "print(xf.shape)\n",
    "x_L1 = nn.Linear(n_emb * block_size, n_hidden, bias=False)\n",
    "print(x_L1.weight.shape)\n",
    "for p in x_L1.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939244\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10              # dimensionality of the character embedding vectors\n",
    "n_hidden = 100          # number of neurons in the hidden layer of the MLP\n",
    "vocab_size = len(trim_vocab) # size of vocabulary database\n",
    "\n",
    "torch.manual_seed(42)   # set seed for reproducibility\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, n_emb), nn.Flatten(1, 2),\n",
    "    nn.Linear(n_emb * block_size, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),             \n",
    "    nn.Linear(          n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "    nn.Linear(          n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "    nn.Linear(n_hidden, vocab_size),                 \n",
    ")\n",
    "\n",
    "# suppress weight inits in linear layers (5/3 is gain associated w/ tanh activation, 0.1 is softmax suppression)\n",
    "with torch.no_grad():\n",
    "    for i in range(len(model)-1):\n",
    "        if isinstance(model._modules[str(i)], nn.Linear):\n",
    "            model._modules[str(i)].weight *= 5/3\n",
    "    model._modules[str(len(model)-1)].weight *= 0.1 \n",
    "\n",
    "# parameters =[p for model._modules[module] in model._modules for p in model._modules[module].parameters()]\n",
    "params=[]\n",
    "for module in model._modules:\n",
    "    params += [p for p in model._modules[module].parameters()]\n",
    "print(sum(p.nelement() for p in params))      # number of parameters in total\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2796,  0.2593,  0.1085,  ...,  0.1572,  0.1119, -0.0568],\n",
       "        [ 0.8167,  0.2437,  0.4262,  ..., -0.0960,  0.9129, -0.2663],\n",
       "        [ 0.0500, -0.1975, -0.0907,  ...,  0.1934, -0.1779, -0.2969],\n",
       "        ...,\n",
       "        [ 0.5243,  0.3323,  0.1363,  ...,  0.5022,  0.2467,  0.1192],\n",
       "        [ 0.3273,  0.1832, -0.0721,  ...,  0.6050,  0.4526, -0.4099],\n",
       "        [ 0.0220, -0.0340, -0.1497,  ..., -0.4129, -0.3204, -0.4504]])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 9.0117\n",
      "Loop duration: 0.0110\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-428-4455c58eb96e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    if i == 0: start = time.time()\n",
    "    if i % 10000 == 0: lstart = time.time()\n",
    "    \n",
    "    # minibatch construct \n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g) # minibatch initialization of batch_size indexes of Xtr\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]                                        # batch X, Y\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # backward pass\n",
    "#     for layer in layers:\n",
    "#         layer.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None                          # zero-out gradients\n",
    "    loss.backward()                            # backpropogate to calculate gradients\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01           # step learning rate decay\n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad                 # perform gradient descent\n",
    "\n",
    "#     if i % 1000 == 0: \n",
    "#         stop = time.time() \n",
    "#         print('Loop duration:', stop - start)\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:                                         # print only every 10000 iterations\n",
    "        print(f'{i:7d}/{max_steps:7d} | Loss: {loss.item():.4f}')\n",
    "        print('Loop duration:', f'{time.time() - lstart:.4f}')\n",
    "        if i > 0: print('Time Elapsed: ', f'{time.time() - start:.4f}', 'ETA:', f'{(time.time() - start)*(max_steps/i):.4f}')\n",
    "    lossi.append(loss.log10().item())                          # track the log of the loss function\n",
    "    \n",
    "#     if i >= 50000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()       # this decorator disables gradient tracking (don't want to update grads when simply calcing loss)\n",
    "def split_loss(split):\n",
    "    x,y = {                                 # create dict of x, y values for each subset\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]                                # slice dict on subset specified by split arg\n",
    "    \n",
    "    # rerun NN (forward pass only) for selected subset to get loss on complete subset of data (not on minibatch)\n",
    "    emb = C[x]                                                  # dims: (N, block_size, n_emb)\n",
    "    z = emb.view(emb.shape[0], -1)                         # concatenation of embedding vectors; (N, block_size*n_emb)\n",
    "    for layer in layers:\n",
    "        z = layer(z)                           # call Linear obj or Tanh obj\n",
    "    loss = F.cross_entropy(z, y)              # loss function\n",
    "    loss.item()\n",
    "    print(split, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4.456291675567627\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deliberately from an office or unlimited .\n",
      "a barrel idea .\n",
      "squeeze or assistance .\n",
      "a insane of desire with a card .\n",
      "apply its solid than the first on by high complex through a theatrical .\n",
      "pass into a travel or societies of .\n",
      "pause or placed as if by hard receiving .\n",
      "a long carriage or function .\n",
      "do advantageous or stages or scrape to affective from 1830 .\n",
      "fall to fill energy or attention in be way .\n",
      "in a angle point .\n",
      "a engaged attention bliss for buildings to evidence in relation to employment identification along a chemical .\n",
      "a commercial warp of something and shaving of the United States perfumes is overeat or design in the use of a piece of advocated equal .\n",
      "an area that carries the signature frame manner .\n",
      "print by bud valley seeing a circular grade of meat the different mentally .\n",
      "come to play a relationship .\n",
      "provide disorder harmful and use .\n",
      "providing or prevalent or shout in small sides or seems .\n",
      "the German brain suddenly .\n",
      "having or covered with constant .\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size                      # initializes with [., ., .]\n",
    "\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]            # iteratively embed the context vector, dimensions: (1, block_size, d)\n",
    "        z = emb.view(1, -1)                         # concatenation of embedding vectors; (N, block_size*n_emb)\n",
    "        for layer in layers:\n",
    "            z = layer(z)                            # call Linear obj or Tanh obj\n",
    "        probs = F.softmax(z, dim=1)                 # softmax transform of L2 activations\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()    # multinom-dist sample with probs probabilities\n",
    "        context = context[1:] + [ix]                # append the sample to the context vector and drop first element of context\n",
    "        out.append(ix)                              # append the sample to the output\n",
    "        \n",
    "        if ix == 0:\n",
    "            break                                   # break while loop when string end identifier is sampled ('.')\n",
    "\n",
    "    print(' '.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 9.300114631652832\n"
     ]
    }
   ],
   "source": [
    "split_loss('val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
