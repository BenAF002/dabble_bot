{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "assert(nltk.download('wordnet'))\n",
    "from nltk.corpus import wordnet as wn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as a\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data will come from Wordnet, a large lexical database of english words and accompanying definitions. We use the natural language toolkit to interact conveniently with wordnet via `nltk.corpus.wordnet`.\n",
    "- Source: https://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (n) the vertical force exerted by a mass as a result of gravity\n",
      "1: (n) sports equipment used in calisthenic exercises and weightlifting; it is not attached to anything and is raised and lowered by use of the hands and arms\n",
      "2: (n) the relative importance granted to something\n",
      "3: (n) an artifact that is heavy\n",
      "4: (n) an oppressive feeling of heavy force\n",
      "5: (n) a system of units used to express the weight of something\n",
      "6: (n) a unit used to measure weight\n",
      "7: (n) (statistics) a coefficient assigned to elements of a frequency distribution in order to represent their relative importance\n",
      "8: (v) weight down with a load\n",
      "9: (v) present with a bias\n"
     ]
    }
   ],
   "source": [
    "# yield sample definitions from word \"weights\"\n",
    "for i,s in enumerate(wn.synsets('weights')):\n",
    "    print(f'{i}: ({s.pos()}) {s.definition()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions in sample: 244,577\n",
      "Distinct words in sampled definitions: 2,343,437\n",
      "Unique words in sampled definitions: 41,151\n"
     ]
    }
   ],
   "source": [
    "# read in sample words (from Kaggle dataset: https://www.kaggle.com/datasets/ruchi798/part-of-speech-tagging/data)\n",
    "wsample = []\n",
    "with open('word_sample.txt', 'r') as file:\n",
    "  for line in file.read().splitlines():\n",
    "    wsample.append(line)\n",
    "\n",
    "# definitions for all words in wsample\n",
    "definitions = [s.definition() for w in wsample for s in wn.synsets(w)]\n",
    "\n",
    "# remove punctuations from definitions and append ' . ' sentence-end token\n",
    "trim_definitions = [''.join(d).translate(str.maketrans('', '', string.punctuation)) + ' . ' for d in definitions]\n",
    "trim_defstring = ''.join(trim_definitions)    # join punctuation-free definitions into a string\n",
    "\n",
    "# create vocab list of all individual words that appear in the sample set of definitions\n",
    "vocab = list(set(sorted(trim_defstring.split())))\n",
    "\n",
    "print(f'Definitions in sample: {len(definitions):,.0f}')\n",
    "print(f'Distinct words in sampled definitions: {len(trim_defstring.split()):,.0f}')\n",
    "print(f'Unique words in sampled definitions: {len(vocab):,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove rare words from the definitions. These are defined as words that appear only once. They tend to be overly specific pronouns or dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [trim_defstring.count(word) for word in vocab]\n",
    "rare_words = {vocab[i] for i,c in enumerate(counts) if c < 2}\n",
    "\n",
    "# remove rare words from the definitions\n",
    "more_trim_defs= []\n",
    "for i,d in enumerate(trim_definitions):\n",
    "    if len(set(d.split()) & rare_words) == 0:\n",
    "        more_trim_defs.append(d)\n",
    "\n",
    "# remove rare words from vocab\n",
    "trim_vocab = list(set(vocab) - set(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34601, 239152, 2279897)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trim_vocab), len(more_trim_defs), len(''.join(more_trim_defs).split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary we have:\n",
    "- 34,601 unique words in the dataset after removing one-offs\n",
    "- 239,152 definitions\n",
    "- 2,279,897 distinct words in the definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare encoding and decoding dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_char = '.'\n",
    "start_char = '<s>'\n",
    "pad_char = '<p>'\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(trim_vocab)}    # word-to-integer mapping dictionary\n",
    "stoi[end_char] = len(stoi) + 1                     # adding end character\n",
    "stoi[start_char] = len(stoi) + 2                   # adding start character\n",
    "stoi[pad_char] = 0                                 # adding pad character\n",
    "itos = {i:s for s,i in stoi.items()}               # integer-to-word mapping dictionary\n",
    "\n",
    "encoder = lambda s: [stoi[c] for c in s]            # encoder\n",
    "decoder = lambda l: ' '.join([itos[i] for i in l])  # decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will naively encode the entire dataset into one tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [encoder(d.split()) for d in more_trim_defs]\n",
    "max_length = max([len(d) for d in data])\n",
    "xdat = [encoder([start_char]) + d[:-1] for d in data]\n",
    "ydat = [d for d in data]\n",
    "\n",
    "# right pad all definitions to max length\n",
    "for d in xdat: d += [0] * (max_length - len(d) + 1)\n",
    "for d in ydat: d += [0] * (max_length - len(d) + 1)\n",
    "\n",
    "xdat = torch.tensor(xdat)\n",
    "ydat = torch.tensor(ydat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "Xt, Yt = xdat[:n], ydat[:n]     # 80% training data\n",
    "Xv, Yv = xdat[n:], ydat[n:]     # 20% validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42);  # stay calm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = len(stoi) + 1\n",
    "batch_size = 128\n",
    "block_size = max_length + 1\n",
    "n_emb = 256           # embedding dimesions\n",
    "n_head = 8            # number of heads per multihead stack (head_size = n_emb // n_head)\n",
    "n_blocks = 8          # number of decoder blocks\n",
    "dropout = 0.2         # probability of zeroing-out neuron in dropouts\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 71]), torch.Size([128, 71]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minibatch(xdat, ydat):\n",
    "    # 1D tensor of random ints between bls and len(xdat) of length bts\n",
    "    idx = torch.randint(len(xdat) - block_size, (batch_size,))\n",
    "\n",
    "    # index into x and y tensors using random ints\n",
    "    x, y = xdat[idx], ydat[idx]\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = minibatch(Xt, Yt)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention is the crux of this whole thing. So, I want to investigate it in more detail before putting together the model's self-attention head.\n",
    "\n",
    "Attention may be thought of as simply producing a weighted average of previous tokens in the training data. \"Previous\" meaning the previous tokens within the context length before each token in the training data. The weights applied to the embedded value of each previous token should be trainable *and* should be informed by the current token's embedded value. For example, if the current token is a noun then we may want the weights applied to preceding adjective to be greater than the weights appliead to preceding nouns. However, if the token is a verb then the weights applied to previous adjectives should be low while the weights applied to previous nouns should be higher.\n",
    "\n",
    "Let's first look at how we can compute these rolling weighted averages over a certain context length in a vectorized format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32  # batch, time (context length), channels\n",
    "\n",
    "weights = torch.ones(T, T)     # initialize tensor of ones (T, T)\n",
    "weights = torch.tril(weights)  # zero-out the upper triangle of the weights matrix\n",
    "weights = weights.masked_fill(weights == 0, float('-inf'))    # fills all 0 elements in tril with '-inf'\n",
    "weights = F.softmax(weights, dim=1)    # softmax transform  over the rows of weights\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through matrix operations and a softmax transformation we've produced a matrix of uniform weights. Note that as each row sums to one, they may be used as weightsghted averages. This weights matrix would apply uniform weights across previous tokens up to eight tokens in the past. Note also that the zeros in the upper triangle ensure that *future tokens* do not influence the weights for the current token.\n",
    "\n",
    "Now, we need to provide a means of producing *trainable* non-uniform weights so that different tokens within the context can provide different weights to the current token. This is achieved by implementing two new matrices: a *key* and a *query* matrix. These will be initialized a linear layers through pytorch so that we can backpropogate through them to tune their weights. A token's query interacts with all the other keys through a dot-product. This dot-product replaces the simple rolling average that we constructed before (although it is the same in form and similar in function).\n",
    "\n",
    "This allows us to train the queries to respond differently to different keys. Certain 'alignments' between a query and a key will be promoted in training, leading to improved conditional predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6431, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3625, 0.1669, 0.4706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2802, 0.2541, 0.2505, 0.2153, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1296, 0.3602, 0.1551, 0.1921, 0.1631, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1554, 0.2850, 0.0848, 0.1227, 0.1966, 0.1555, 0.0000, 0.0000],\n",
       "        [0.0807, 0.1930, 0.2052, 0.0929, 0.1514, 0.1391, 0.1377, 0.0000],\n",
       "        [0.1535, 0.0653, 0.2159, 0.1004, 0.0793, 0.2435, 0.0539, 0.0883]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B, T, C)  # initialize some random input data\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)    # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1)  # transpose of the last two dimensions of k (B, T, 16) --> (B, 16, T) --> q @ k (B, T, T)\n",
    "weights = weights * head_size**-0.5\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))   # zero-out future positions\n",
    "weights = F.softmax(weights, dim=-1)    # softmax over the last dimension of weights\n",
    "\n",
    "weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6608,  0.4298,  0.1429,  1.3202,  1.0733,  1.0446,  0.9047, -0.2830,\n",
       "        -0.0591,  1.3761,  0.9947, -1.7909, -0.2628,  1.9571,  0.0146, -0.3971,\n",
       "        -0.2107,  0.9406,  0.3464, -1.3773, -0.8786, -0.3244,  0.4048,  0.1354,\n",
       "         0.3485,  0.0094,  1.5291, -0.6088, -0.9659,  1.5619, -0.5781,  0.9003],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights @ x)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically `x` will not be multiplied with the weights matrix directly, but through a *value* linear layer that is unique to the specific attention head. One benefit of this is that it aligns the shape of the weighted attention output with the head size rather than relying on the shape of the input data. E.g.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre linear layer:  torch.Size([4, 8, 32])\n",
      "Post linear layer:  torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "print('Pre linear layer: ', (weights @ x).shape)\n",
    "print('Post linear layer: ', (weights @ value(x)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the value layer is more significant. The value matrix applies another set of trainable weights to the input data such that the resulting matrix of $xV$ has contextual information from every other element in $x$ for each elementy in $x$. Masking the upper triangle with zeros via the weights matrix removes the contextual information from future elements. For example, this enables more meaningful weights to be applied not just to a noun like \"wood\" but to a noun in the context of its sentence, e.g. \"mossy dark wood\". This context is the vectorized representation of wood with values that encode information about the words (tokens) that precede it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a simple standardization step that is included in the *\"Attention is all You Need\"* (2017) paper. That is, to scale the weights by the inverse of the square of the head size prior to applying the softmax transformation. From the paper:\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$ \n",
    "Where $Q$ is the query matrix, $K$ is the key matrix, $V$ is the value matrix, and $d_k$ is the head size.\n",
    "\n",
    "This is done to standardize the weights before applying the softmax, promoting stability. Let's look briefly at the variance of the weights before and after this transformation for `k` and `q` both with unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k var 1.0249735116958618\n",
      "q var 0.9907991290092468\n",
      "16.46785545349121\n",
      "1.0292409658432007\n"
     ]
    }
   ],
   "source": [
    "# k = torch.randn(B, T, head_size)\n",
    "# q = torch.randn(B, T, head_size)\n",
    "\n",
    "print('k var', k.var().item())\n",
    "print('q var', q.var().item())\n",
    "\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "print(weights.var().item())\n",
    "\n",
    "print((weights * head_size**-0.5).var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the $\\frac{1}{\\sqrt{d_k}}$ modification ensures that the scale of the variance of the weights matrix is on the same order of magnitude as the scale of the variances for the query and key matrices. This is important for the softmax transformation because the softmax will exaggerate the probabilities associated with large positive values. At initialization, if we happen to have very large positive values, then a feedback-loop may be created wherein the softmax transformed weights matrix converges to a one-hot-encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Notes on Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention as a Communication Mechanism** \\\n",
    "All of these tokens are like nodes in a directed acyclic graph. Each node aggregates the information from every other node that points towards it (not the future nodes) through a weightsghted sum with data-dependent weights.\n",
    "\n",
    "**On Spatial Information** \\\n",
    "The data-dependent weightsghted averages that drive this attention mechanism provide no information about the position of a node (token). The value of the weightsghted average doesn't change if we change the ordering of the values and their weights. This is why we encode position information into a positional embedding space.\n",
    "\n",
    "**On Batch Dependence** \\\n",
    "Elements in each batch sample are completely independent of elements in other batch samples.\n",
    "\n",
    "**\"Encoder\" Attention Block** \\\n",
    "Encoder attention blocks remove the constraints on temporal information sharing (the tril() statement) and allow all nodes to talk to eachother. In this case the graph could be cyclic.\n",
    "\n",
    "The attention block that we have created is called a \"Decoder\" because it has the triangular masking. Decoder attention blocks are widely seen in autoregressive applications like this one.\n",
    "\n",
    "**\"Self Attention\" vs \"Cross Attention\"** \\\n",
    "Self attention just means that the keys, queries, and values all come from the same source - `x`. Cross attention provides keys and values from different sources, like encoder blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the single self-attention head used in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # (T, T)\n",
    "            # torch.tril() is not a parameter, so we have to use register_buffer to assign it to the module\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B, T, hs)\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # (B, T, T); scaled by 1/sqrt(hs)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)        \n",
    "        \n",
    "        # aggregate values by weights\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* This summary involves a lot of discussion about embedding and embedding spaces without explaining those concepts. It's really more for organizing my thoughts than anything and isn't meant to be a rigorous explanation.\n",
    "\n",
    "Let's summarize some things first. Within an attention head *i* we have a distinct pair of key $K^{(i)}$ and query $Q^{(i)}$ matrices. Which produce a distinct attention pattern, a weights matrix $W_{K,Q}^{(i)}$. This attention matrix is multiplied by a sequence of contextualized value vectors unique to the head, $V^{(i)}$. The resulting matrix, $W_V^{(i)}$ may be thought of as a set of column vectors. Each column vector applies a transformation to its corresponding column in the embedded input data, moving it in the embedding space. \n",
    "\n",
    "Now, when we parrallelize this process across mulitple attention heads, we yield multiple transformation matrices. These transformation matrices may move the input data in different ways. For example, for a vector of the token \"ball\", one transformation may point in directions of \"sports\", \"activities\", \"toys\", and so on, while another points in directions of \"events\", \"dining\", \"galas\"... The multiple heads can attenuate on different contextual meanings of tokens more specifically than one head alone could. In as sense, we're splitting our attention. ;)\n",
    "\n",
    "Essentially, we are enabling the model to better learn the many distinct ways in which context may change meaning.\n",
    "\n",
    "The transformations from the multiple heads can then be aggregated to yield final predictions. In the paper, the results of each attention head are simply concatenated together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for h in range(num_heads)])    # create list of heads\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_emb)    # linear transformation of the output from the head stack\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)    # feed forward through heads and concatenate output\n",
    "        out = self.dropout(self.proj(out))                     # pass output through linear layer and apply dropout\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After passing through the multi-headed attention stack we will feed forward through a fairly simple MLP. This replicates the MLP architecture in the *\"Attention is all You Need\"* (2017) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),     # mult 4 bc the paper does a 4x channel expansion in the feedforward\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll wrap up the multi-head attention stack and the feed-forward layer into a decoder block. This blocks can be conveniently chained together to augment the model's length. Each block is fundamentally just the masked multi-headed attention stack and the feed-forward layer from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.sa = MultiHead(n_head, head_size)  # self-attention stack\n",
    "        self.ffwd = FeedForward(n_emb)          # feed-forward layer\n",
    "        self.ln1 = nn.LayerNorm(n_emb)   # layer normalization for self-attention stack\n",
    "        self.ln2 = nn.LayerNorm(n_emb)   # layer normalization for feed forward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.ln1(x + self.sa(x))     # residual self-attention stack connection\n",
    "        # x = self.ln2(x + self.ffwd(x))   # residual feed-forward connection\n",
    "        x = x + self.sa(self.ln1(x))     # residual self-attention stack connection\n",
    "        x = x + self.ffwd(self.ln2(x))   # residual feed-forward connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:\n",
    "Note: The paper applies dropout to the embeddings before forwarding through the model; I ignore that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DabbleBot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)      # token embedding\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)   # positional embedding\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for b in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_emb)                # final layer norm\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)    # output linear layer\n",
    "        \n",
    "        \n",
    "    def forward(self, input, targets=None):\n",
    "        B, T = input.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers                    dimension tracking:\n",
    "        tok_emb = self.token_embedding_table(input)                               # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))   # (T,C)\n",
    "        x = tok_emb + pos_emb                                                     # (B,T,C)\n",
    "        x = self.blocks(x)                                                        # (B,T,C)\n",
    "        x = self.ln_f(x)                                                          # (B,T,C)\n",
    "        logits = self.lm_head(x)                                                  # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=0)    # ignore index of pad_char\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, samples):    # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        self.eval()\n",
    "        sample = []\n",
    "        \n",
    "        for s in range(samples):\n",
    "            ctx = idx\n",
    "            \n",
    "            while True:\n",
    "                ctx_cond = ctx[:, -block_size:]\n",
    "                logits, loss = self(ctx_cond)\n",
    "\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                ctx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "                # append sampled index to the running sequence\n",
    "                ctx = torch.cat((ctx, ctx_next), dim=1) # (B, T+1)\n",
    "                \n",
    "                if ctx_next.item() == stoi[end_char] or ctx.shape[1] > 50:\n",
    "                    break\n",
    "            sample.append(decoder(ctx.tolist()[0]))\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, iters):\n",
    "    out = {}\n",
    "    data = {'train': (Xt, Yt), 'val': (Xv, Yv)}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(iters)\n",
    "        for k in range(iters):\n",
    "            X, Y = minibatch(*data[split])\n",
    "            logits, loss = model.forward(input=X, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = DabbleBot()\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_iters = 10000\n",
    "eval_iters = 500\n",
    "tloss, vloss = [], []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # paper uses Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24,082,476 total parameters'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data.shape.numel())\n",
    "        param_count += param.data.shape.numel()\n",
    "f\"{param_count:,.0f} total parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check in\n",
      "losses checked\n",
      "waaaaa\n",
      "step 0: train loss 10.5989, val loss 10.5344               | ETA: 558.79 min\n",
      "check in\n",
      "losses checked\n",
      "waaaaa\n",
      "step 500: train loss 4.9537, val loss 5.7206               | ETA: 544.95 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[396], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mxb, targets\u001b[38;5;241m=\u001b[39myb)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m run_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\benak\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benak\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benak\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    start = time.time()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_iters == 0:\n",
    "        \n",
    "        losses = estimate_loss(model, eval_iters)\n",
    "        tloss.append(losses['train'])\n",
    "        vloss.append(losses['val'])\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} \\\n",
    "              | ETA: {run_time / 60 * (max_iters-iter):.2f} min\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = minibatch(Xt, Yt)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(input=xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    run_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
